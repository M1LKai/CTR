数据处理
  1，探索性分析
    本文的数据来自于2018年科大讯飞AI营销算大赛的初赛以及复赛数据，本次大赛提供了科大讯飞AI营销云的海量的广告数据和用户数据，
  本论文将结合此数据讨论如何有效利用这些数据去通过人工智能技术构建预测模型预估用户的广告点击概率，
  即给定广告点击相关的广告、媒体、用户、上下文内容等信息的条件下预测广告点击概率。
    数据内容主要分为两个两个量级不同的数据集，其中包括包含用户id的基本信息，还有广告信息类、媒体信息类、用户信息类、上下文信息类。
  首先对数据进行基本处理，读取并将初复赛的数据融合，一共包含有35个字段的3080276条记录，其中有bool类型字段6个，float类型字段2个, 
  int类型字段18, object类型9个；其中训练集与测试集之间没有交集，训练集正负样本比例大致为1/4（如图）；整个数据集中user_tags，make，model，osv，app_cate_id，f_channel，app_id这7个字段含有缺失值。
    接下来对每个字段进行单独分析处理。首先针对广告id进行处理，训练集内删去重复值后有2187个广告id，
  共有3000000条关于adid的记录，最多一个广告id有124653条记录，最低1条，平均每个广告id含有1372条记录。
  通过对广告记录数对应广告量的散点图（figure1）分析，大部分广告还是集中在点击量比较少的区域；
  训练集拥有39个广告主id的3000000条记录，分析发现广告主的点击率之间相差较大，但是不同广告主的曝光次数也差异比较大不可确定差异是否与之有关，
  故进行卡方检验，可得P值小于0.05，有显著的统计学意义，即广告主对用户点击率有积极的影响作用；
  训练集时间字段共有2187，最大的重复字段为62个，但是由于过于但是时间字段与点击率相关的主要为hour部分，所以进行更细致的切分，
  通过曝光量与点击量的时间柱形图，可以看出与每天时间变化存在一定的关系。
    其他字段处理还包括：908个订单id与其他7个字段对应；活动id与广告主id唯一对应；34个广告主名称对应38个广告主id，但都不是有缺失的特征
  广告主名称与广告主id不是一一对应，id底下唯一对应一个名字，名字不唯一对应一个id，所以一个广告主名字可能有多个id；创意高和创意宽都会
  影响到点击率的大小；也有很多的相关点击率是100%，明显是由于基数过大与过少，这样的数据不具有泛化能力；
  手机品牌与机型太多，粒度太细容易过拟合，所以在后续操作中可能采取合并手机类型的做法。
；还包括其他一些的处理，有些一对一的特征，在之后的预测过程中可以考虑删去其中之一。
  2，数据预处理
     2.1数据清洗 Data Cleansing
          数据合并后删去重复数据
          首先对time进行分箱
          对操作系统及其版本、名称进行处理，版本脏数据统一，清洗，对品牌的数据清洗，对model清洗
           添加cross_feature
          fillna处理缺失值
          把布尔型特征更改为数值型,replace函数将特征数值化
          labelencoder对除用户标签以外的类别特征进行Labelencoder编码

特征工程
  1，特征选择方法介绍
        
       
  2，本论文特征选择
         nunique特征
         user_tags CountVectorize
         add nunique feature
         add ctr feature
      
算法模型
  1，CTR常用模型介绍
        lr(Logistic Regression)
        fm与ffm
        GBDT
        基于FTRL的online learning
        MLR
        DIN
  2，本论文采用模型
        本论文首先采用较为传统的模型进行训练。第一采用了曾经在泰坦尼克号事件相关预测所使用过的随机森林模型进行尝试，
      然后有使用的CTR预测的基础模型逻辑回归进行模型训练及预测。两次尝试最终的效果都比较差，与预期有较大的差距，并且处理
      较大规模数据速度太慢，在CTR预测中不太适用。
        根据尝试结果及其他模型的学习结论，综合考虑较好的数据处理效率和准确性，决定采用在大量预测点击率和搜索排序都
      表现不错的GBDT模型进行训练，GBDT采用Boosting思想每次分类都会迭代产生更强的分类器，并且还具有比较好的记忆能力和特征筛选能力，
      所以在本次CTR预测的实例中表现不错。而在GBDT的工程实现中，则是选择了Light Gradient Boosting Machine (LightGBM)，
      它是一个由微软亚洲研究院分布式机器学习工具包（DMTK）团队开源的基于GBDT的框架。
      lightGBM不仅仅可以处理大规模数据，甚至还有更快的训练速度和效率，对于内存的使用也比较小，
      最吸引人的是最终模型的准确率也是有保障，同时还支持并行化的学习。
          lightGBM主要是进行了重心位于模型训练速度的优化。首先，lightGBM采用单边梯度采样算法(Grandient-based One-Side Sampling，GOSS)
       LightGBM使用GOSS算法对训练样本采样过程进行优化。在GBDT的基本算法中是没有关于样本的权重，
       所以LightGBM采用了基于每一个样本的梯度进行训练样本的优化，数据拥有较大梯度的时候对计算信息增益的贡献比较大。
       当一个样本点的梯度很比较小的时候，就说明该样本的训练误差其实很小，即该样本已经被充分训练。
       然而如果单纯在计算过程中，仅仅保留梯度较大的样本，抛弃梯度较小样本，这样的做法会改变样本的分布并且降低学习的精度。GOSS算法的提出就是针对这个问题，
       GOSS算法的基本思想是根据梯度首先对训练集数据排序，并且预设一个恰当的比例，保留在所有样本中梯度高于的数据样本；
       梯度低于该比例的数据样本也不会被直接丢弃，而是设置一个采样比例，从梯度较小的样本中按比例抽取样本。
       同时为了弥补对样本分布造成的影响，GOSS算法在计算信息增益时，会对较小梯度的数据集乘以一个系数，用来放大。
       这样，在计算信息增益时，算法就会去更加注意还未被充分训练的样本数据；
       除此之外，lightGBN还有一个很重要的算法Exclusive Feature Bundling算法(EFB)。lightGBM不仅优化了采集样本的过程，还进行特征抽取，
       特征抽取与特征提取不同，特征抽取并不减少训练时数据特征向量的维度，而是通过将互斥特征通过一定的算法绑定在一起，从而减少特征维度。
       数据中互斥特征被绑定在一起后，就会形成低维的特征集合，这使得模型能够有效的避免那些对0值特征的不必要计算。
       实际上，在算法中，这就可以有效的降低创建直方图的时间复杂度，从而快速过渡到下一个优化算法直方图算法（Histogram算法),相对于极端梯度提升（eXtreme Gradient Boosting，XGBoost）
       采用的需要进行提前排序的exact算法，Histogram算法能减少内存消耗，还能做减差加速作用。
       lightGBM中一个子节点的直方图可以通过父节点的直方图减去兄弟节点的直方图即可得到，从而实现加速，并且实际的数据集上表明，
       离散化的分裂点对最终学习的精度影响并不大，甚至会更好一些。
       因为这里的决策树本身就是弱学习器，采用Histogram离散化特征值反而会起到正则化的效果，从而提高算法的泛化能力；lightGBM采用的
       分支策略为按叶子生长（leaf-wise）的策略，相对于大多数按层生长（level-wise)的策略，这种策略更加高效，
       该策略每次从当前决策树所有的叶子节点中，找到分裂增益最大的一个叶子节点，然后分裂，如此循环往复。
       这样的机制下，减少了对增益较低的叶子节点的分裂计算，减少不必要的开销。
       与leve-wise的策略相比，在分裂次数相同的情况下，leaf-wise对误差处理效果更好，能有效降低误差得到更好的精度。
       Leaf-wise算法有一个缺点就是可能会生成较深的决策树，因此，LightGBM在Leaf-wise上增加了限制最大深度的参数，既能保证算法高效的同时，还能防止过拟合。
       


      lightGBM实现GBDT，五折交叉训练
      减小内存
      hstack CountVectorizer
      利用不同折数加参数，特征，样本（随机数种子）扰动
      加权平均/调和平均  得到最终成绩
总结与思考

